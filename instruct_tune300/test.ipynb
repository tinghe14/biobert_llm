{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de1bc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yhu5/.conda/memorization/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-22 11:21:12,511\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 11:21:13 config.py:904] Defaulting to use mp for distributed inference\n",
      "INFO 10-22 11:21:13 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='./models/Llama-3-8b-mix-2epoch/merged/', speculative_config=None, tokenizer='./models/Llama-3-8b-mix-2epoch/merged/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./models/Llama-3-8b-mix-2epoch/merged/, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 10-22 11:21:13 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-22 11:21:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-22 11:21:15 utils.py:981] Found nccl from library libnccl.so.2\n",
      "INFO 10-22 11:21:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:15 utils.py:981] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:15 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-22 11:21:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/yhu5/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/yhu5/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "INFO 10-22 11:21:15 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fb1c18f18d0>, local_subscribe_port=57906, remote_subscribe_port=None)\n",
      "INFO 10-22 11:21:16 model_runner.py:997] Starting to load model ./models/Llama-3-8b-mix-2epoch/merged/...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:16 model_runner.py:997] Starting to load model ./models/Llama-3-8b-mix-2epoch/merged/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.33s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 11:21:20 model_runner.py:1008] Loading model weights took 7.4829 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:21 model_runner.py:1008] Loading model weights took 7.4829 GB\n",
      "INFO 10-22 11:21:22 distributed_gpu_executor.py:57] # GPU blocks: 61995, # CPU blocks: 4096\n",
      "INFO 10-22 11:21:23 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-22 11:21:23 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:25 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:25 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-22 11:21:42 custom_all_reduce.py:223] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:42 custom_all_reduce.py:223] Registering 2275 cuda graph addresses\n",
      "INFO 10-22 11:21:42 model_runner.py:1430] Graph capturing finished in 19 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=50439)\u001b[0;0m INFO 10-22 11:21:42 model_runner.py:1430] Graph capturing finished in 17 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpus = '1,2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "\n",
    "model_name = './models/Llama-3-8b-mix-2epoch/merged/'\n",
    "\n",
    "device_map = [f\"cuda:{i}\" for i in gpus.split(\",\")]\n",
    "\n",
    "from vllm import LLM,SamplingParams\n",
    "sampling_params = SamplingParams(max_tokens=512,stop='<EOS>',temperature=0)\n",
    "llm = LLM(model=f\"{model_name}\", tensor_parallel_size=len(device_map))  # Create an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2a5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "prompt = '''### Task:\n",
    "Your task is to generate an HTML version of an input text, using HTML <span> tags to mark up specific entities.\n",
    "\n",
    "### Entity Markup Guides:\n",
    "Use <span class=\"problem\"> to denote a medical problem.\n",
    "Use <span class=\"treatment\"> to denote a treatment.\n",
    "Use <span class=\"test\"> to denote a test.\n",
    "Use <span class=\"drug\"> to denote a drug.\n",
    "\n",
    "### Entity Definitions:\n",
    "Medical Problem: The abnormal condition that happens physically or mentally to a patient.\n",
    "Treatment: The procedures, interventions, and substances given to a patient for treating a problem.\n",
    "Drug: Generic or brand name of a single medication or a collective name of a group of medication.\n",
    "Test: A medical procedure performed (i) to detect or diagnose a problem, (ii) to monitor diseases, disease processes, and susceptibility, or (iii) to determine a course of treatment.\n",
    "\n",
    "### Input Text: {} <EOS>\n",
    "### Output Text:'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6e9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_list(input_list, batch_size):\n",
    "    batched_list = []\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        batched_list.append(input_list[i:i + batch_size])\n",
    "    return batched_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316e0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "import time\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    nvmlInit()\n",
    "    device_count = nvmlDeviceGetCount()  # Get number of GPUs\n",
    "    memory_usage = []\n",
    "    for i in [5,6]:\n",
    "        handle = nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        memory_usage.append(info.used / 1024 ** 2)  # Convert to MB\n",
    "    return memory_usage\n",
    "\n",
    "def get_gpu_power_usage():\n",
    "    device_count = nvmlDeviceGetCount()  # Get number of GPUs\n",
    "    power_usage = []\n",
    "    for i in [5,6]:\n",
    "        handle = nvmlDeviceGetHandleByIndex(i)\n",
    "        power = nvmlDeviceGetPowerUsage(handle)  # Returns power in milliwatts\n",
    "        power_usage.append(power / 1000)  # Convert to watts\n",
    "    return power_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de94a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def group_relations(tuples_set):\n",
    "    grouped_tuples = {}\n",
    "    for pair in tuples_set:\n",
    "        key = pair[0]  # The first tuple of the pair\n",
    "        if key in grouped_tuples:\n",
    "            grouped_tuples[key].append(pair[1])\n",
    "        else:\n",
    "            grouped_tuples[key] = [pair[1]]\n",
    "            \n",
    "    # Sorting the values in each group in descending order based on the second value of the tuple\n",
    "    sorted_grouped_tuples = {}\n",
    "    for key, values in grouped_tuples.items():\n",
    "        sorted_grouped_tuples[key] = sorted(values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_grouped_tuples\n",
    "\n",
    "def replace_entities_with_types(sent, entities):\n",
    "    sent_text = str(sent)\n",
    "    if isinstance(entities, list):\n",
    "        for e in entities:\n",
    "            ent_type, start, end =e\n",
    "            sent_text = sent_text[:start - sent.start_char]+f'<span class=\"{ent_type}\">{sent_text[start - sent.start_char:end - sent.start_char]}</span>'+sent_text[end - sent.start_char:] \n",
    "    else:\n",
    "        ent_type, start, end =entities\n",
    "        sent_text = sent_text[:start - sent.start_char]+f'<span class=\"{ent_type}\">{sent_text[start - sent.start_char:end - sent.start_char]}</span>'+sent_text[end - sent.start_char:] \n",
    "    return sent_text\n",
    "\n",
    "def sentence_relations(text,entities, relations, nlp):\n",
    "    test_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "    \n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"labvalue\"> to denote a numeric value or a normal description of the result of a lab test.\n",
    "    Use <span class=\"reference_range\"> to denote the range or interval of values that are deemed as normal for a test in a healthy person.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a test.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    drug_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"form\"> to denote the form of drug.\n",
    "    Use <span class=\"frequency\"> to denote the frequency of taking a drug.\n",
    "    Use <span class=\"dosage\"> to denote the amount of active ingredient from the number of drugs prescribed.\n",
    "    Use <span class=\"duration\"> to denote the time period a patient should take a drug.\n",
    "    Use <span class=\"strength\"> to denote the amount of active ingredient in a given dosage form.\n",
    "    Use <span class=\"route\"> to denote the way by which a drug, fluid, poison, or other substance is taken into the body.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a drug.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "\n",
    "    problem_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"uncertain\"> to denote a measure of doubt.\n",
    "    Use <span class=\"condition\"> to denote a phrase that indicates the problems existing in a certain situation.\n",
    "    Use <span class=\"subject\"> to denote the person entity who is experiencing the disorder.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"bodyloc\"> to denote the location on the body where the observation is present.\n",
    "    Use <span class=\"severity\"> to denote the degree of intensity of a clinical condition.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a problem.\n",
    "    Use <span class=\"course\"> to denote the development or alteration of a problem.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    treatment_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a treatment.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    df = pd.DataFrame(columns=['main_entity','unprocessed', 'processed'])\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentence_relations = []\n",
    "\n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    main_entities = []\n",
    "    for sent in sentences:\n",
    "        sent_relations = set()\n",
    "        sent_entities = []\n",
    "\n",
    "        # Check if entities are in the current sentence\n",
    "        for ent_id, ent in entities.items():\n",
    "            if ent[1] >= sent.start_char and ent[2] <= sent.end_char:\n",
    "                sent_entities.append(ent)\n",
    "                                \n",
    "        # Check for existing relations in the current sentence\n",
    "        for rel in relations:\n",
    "            rel_type, ent1, ent2 = rel\n",
    "            if ent1 in sent_entities and ent2 in sent_entities:\n",
    "                sent_relations.add((ent1, ent2))\n",
    "        sent_relations = group_relations(sent_relations)\n",
    "        \n",
    "        for main_entity, modifier_entities in sent_relations.items():\n",
    "            #print (main_entity[0])\n",
    "            main_entities.append(main_entity[0])\n",
    "            # Replace entity mentions with their types\n",
    "            modifier_sent_text = replace_entities_with_types(sent, modifier_entities).replace('\\n',' ')\n",
    "            main_entity_sent_text = replace_entities_with_types(sent, main_entity).replace('\\n',' ')\n",
    "            \n",
    "            if main_entity[0] == 'problem':\n",
    "                unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'drug':\n",
    "                unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'treatment':\n",
    "                unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'test':\n",
    "                unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "        # check for non-existing relations in current sentence\n",
    "        for entity in sent_entities:\n",
    "            if entity not in sent_relations and entity[0] in ['problem','treatment','test','drug']:\n",
    "                #print (entity[0])\n",
    "                main_entities.append(entity[0])\n",
    "                main_entity_sent_text = replace_entities_with_types(sent, entity).replace('\\n',' ')\n",
    "                sent_text = str(sent).replace('\\n',' ')\n",
    "                if entity[0] == 'problem':\n",
    "                    unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'drug':\n",
    "                    unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'treatment':\n",
    "                    unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'test':\n",
    "                    unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "    df = pd.concat([df, pd.DataFrame({'main_entity':main_entities,'unprocessed': unprocessed, 'processed': processed})], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def read_brat_files(txt_path, ann_path):\n",
    "    with open(txt_path, \"r\",encoding='utf-8') as txt_file:\n",
    "        text = txt_file.read()\n",
    "    \n",
    "    entities = {}\n",
    "    relations = []\n",
    "    \n",
    "    new_lines = reorg_annfile(ann_path)\n",
    "    for line in new_lines:\n",
    "        try:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0].startswith('T'):\n",
    "                try:\n",
    "                    ent_id, ent_info = parts[0], parts[1]\n",
    "                    ent_type, start, end = ent_info.split(' ')\n",
    "                    ent_type = ent_type.lower()\n",
    "                    entities[ent_id] = (ent_type, int(start), int(end))\n",
    "                except:\n",
    "\n",
    "                    print (new_lines)\n",
    "\n",
    "                    raise\n",
    "            elif parts[0].startswith('R'):\n",
    "                rel_id, rel_type, arg1, arg2 = [parts[0]]+parts[1].split(' ')\n",
    "                relations.append((rel_type, entities[arg1.split(':')[1]], entities[arg2.split(':')[1]]))\n",
    "        except:\n",
    "            continue\n",
    "    return text, entities, relations\n",
    "\n",
    "def follows_pattern(s):\n",
    "    # Define the regex pattern\n",
    "    pattern = r'^[TR]\\d+'\n",
    "    \n",
    "    # Use re.match to check if the string matches the pattern\n",
    "    if re.match(pattern, s):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def reorg_annfile(file):\n",
    "    new_file=[]\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    skip = False\n",
    "    for i,line in enumerate(lines):\n",
    "        if i <= len(lines)-2:\n",
    "            if not follows_pattern(lines[i+1]):\n",
    "                new_file.append(line.strip()+' '+lines[i+1].strip()+'\\n')\n",
    "                skip = True\n",
    "            else:\n",
    "                if not skip:\n",
    "                    new_file.append(line)\n",
    "                else:\n",
    "                    skip = False\n",
    "                    pass\n",
    "                \n",
    "        else:\n",
    "            if not skip:\n",
    "                new_file.append(line)\n",
    "            else:\n",
    "                skip = False\n",
    "                pass\n",
    "    return new_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acff4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i2b2\n",
      "Average GPU memory usage (per GPU): [73329.25, 73329.25]\n",
      "Total GPU time: 5.51078724861145 seconds (~0.00 hours)\n",
      "Average GPU power consumption (per GPU): [294.42699999999996, 307.4886666666667]\n",
      "Total energy consumption (per GPU): [0.4473378883051873, 0.46597867640925783] Wh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "separator = '\\t'\n",
    "batch_size = 100\n",
    "for dataset in ['i2b2']:\n",
    "    print(dataset)\n",
    "    files = glob(f'../data/test/{dataset}/*.bio')\n",
    "\n",
    "    prompts = []\n",
    "    for i, file in enumerate(files):\n",
    "        with open(file, 'r', encoding='utf-8') as f_read:\n",
    "            text = ' '.join([line.split(separator)[0] for line in f_read.read().splitlines()])\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        prompts.append(prompt.format(text))\n",
    "\n",
    "    prompts_list = batch_list(prompts, batch_size)\n",
    "\n",
    "    outputs = []\n",
    "    # Initialize tracking variables\n",
    "    gpu_memory_usage_per_batch = []\n",
    "    gpu_time_per_batch = []\n",
    "    gpu_power_usage_per_batch = []\n",
    "\n",
    "    # Start your batch processing loop\n",
    "    for i, prompt_list in enumerate(prompts_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Record initial GPU power usage\n",
    "        initial_power = get_gpu_power_usage()\n",
    "\n",
    "        # Generate the output\n",
    "        output = llm.generate(prompt_list, sampling_params, use_tqdm=False)\n",
    "\n",
    "        # Record final GPU power usage\n",
    "        final_power = get_gpu_power_usage()\n",
    "\n",
    "        end_time = time.time()\n",
    "        gpu_time = end_time - start_time\n",
    "        gpu_time_per_batch.append(gpu_time)\n",
    "\n",
    "        # Record GPU memory usage and power consumption after the batch\n",
    "        gpu_memory_used = get_gpu_memory_usage()\n",
    "        gpu_memory_usage_per_batch.append(gpu_memory_used)\n",
    "\n",
    "        # Calculate average power usage for each GPU\n",
    "        avg_power_usage = [(init + final) / 2 for init, final in zip(initial_power, final_power)]\n",
    "        gpu_power_usage_per_batch.append(avg_power_usage)\n",
    "\n",
    "        outputs += output\n",
    "\n",
    "    # Calculate average GPU memory usage across all GPUs\n",
    "    if gpu_memory_usage_per_batch:\n",
    "        avg_gpu_memory_usage = [sum(mem) / len(gpu_memory_usage_per_batch) for mem in zip(*gpu_memory_usage_per_batch)]\n",
    "        print(f'Average GPU memory usage (per GPU): {avg_gpu_memory_usage}')\n",
    "\n",
    "    # Calculate total GPU time\n",
    "    total_gpu_time = sum(gpu_time_per_batch)\n",
    "    print(f'Total GPU time: {total_gpu_time} seconds (~{total_gpu_time / 3600:.2f} hours)')\n",
    "\n",
    "    # Calculate average power consumption per GPU\n",
    "    if gpu_power_usage_per_batch:\n",
    "        avg_gpu_power_usage = [sum(power) / len(gpu_power_usage_per_batch) for power in zip(*gpu_power_usage_per_batch)]\n",
    "        print(f'Average GPU power consumption (per GPU): {avg_gpu_power_usage}')\n",
    "\n",
    "    # Calculate total energy consumption per GPU\n",
    "    total_energy_consumption = [\n",
    "        sum([power[i] * time / 3600 for power, time in zip(gpu_power_usage_per_batch, gpu_time_per_batch)])\n",
    "        for i in range(len(avg_gpu_power_usage))\n",
    "    ]\n",
    "    print(f'Total energy consumption (per GPU): {total_energy_consumption} Wh')\n",
    "    \n",
    "    for i,seq in enumerate(outputs):\n",
    "        file_name = files[i].split('/')[-1].split('.')[0]\n",
    "        with open(f'./output/NER/{file_name}.html','w',encoding='utf-8') as f_write:\n",
    "            f_write.write(seq.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000180ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "Average GPU memory usage (per GPU): [73329.25, 73329.25]\n",
      "Total GPU time: 12.960270166397095 seconds (~0.00 hours)\n",
      "Average GPU power consumption (per GPU): [324.594375, 339.533]\n",
      "Total energy consumption (per GPU): [1.1455124690044256, 1.2499108208171528] Wh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in ['i2b2_test']:\n",
    "    # Load the SpaCy model for sentence tokenization\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    txt = glob(f'../data/RE/test_LLM/*.txt')\n",
    "\n",
    "    relation_types = []\n",
    "    entity_pairs = []\n",
    "    df = pd.DataFrame(columns=['main_entity','unprocessed', 'processed'])\n",
    "    for txt_path in txt:\n",
    "        filename = txt_path.split('/')[-1].split('.')[0]\n",
    "        ann_path = f'../data/RE/test_LLM/{filename}.ann'\n",
    "\n",
    "        # Read the Brat files and extract the entities and relations\n",
    "        text, entities, relations = read_brat_files(txt_path, ann_path)\n",
    "\n",
    "        # Extract sentence relations\n",
    "        df = pd.concat([df, sentence_relations(text, entities, relations, nlp)])\n",
    "    \n",
    "    print (len(df))\n",
    "        \n",
    "    unprocessed = []\n",
    "    for index, row in df.iterrows():\n",
    "        unprocessed.append(row['unprocessed'])\n",
    "        \n",
    "\n",
    "    batch_size = 100\n",
    "    separator = '\\t'\n",
    "    \n",
    "    prompts_list = batch_list(unprocessed, batch_size)\n",
    "\n",
    "    outputs = []\n",
    "    # Initialize tracking variables\n",
    "    gpu_memory_usage_per_batch = []\n",
    "    gpu_time_per_batch = []\n",
    "    gpu_power_usage_per_batch = []\n",
    "\n",
    "    # Start your batch processing loop\n",
    "    for i, prompt_list in enumerate(prompts_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Record initial GPU power usage\n",
    "        initial_power = get_gpu_power_usage()\n",
    "\n",
    "        # Generate the output\n",
    "        output = llm.generate(prompt_list, sampling_params, use_tqdm=False)\n",
    "\n",
    "        # Record final GPU power usage\n",
    "        final_power = get_gpu_power_usage()\n",
    "\n",
    "        end_time = time.time()\n",
    "        gpu_time = end_time - start_time\n",
    "        gpu_time_per_batch.append(gpu_time)\n",
    "\n",
    "        # Record GPU memory usage and power consumption after the batch\n",
    "        gpu_memory_used = get_gpu_memory_usage()\n",
    "        gpu_memory_usage_per_batch.append(gpu_memory_used)\n",
    "\n",
    "        # Calculate average power usage for each GPU\n",
    "        avg_power_usage = [(init + final) / 2 for init, final in zip(initial_power, final_power)]\n",
    "        gpu_power_usage_per_batch.append(avg_power_usage)\n",
    "\n",
    "        outputs += output\n",
    "\n",
    "    # Calculate average GPU memory usage across all GPUs\n",
    "    if gpu_memory_usage_per_batch:\n",
    "        avg_gpu_memory_usage = [sum(mem) / len(gpu_memory_usage_per_batch) for mem in zip(*gpu_memory_usage_per_batch)]\n",
    "        print(f'Average GPU memory usage (per GPU): {avg_gpu_memory_usage}')\n",
    "\n",
    "    # Calculate total GPU time\n",
    "    total_gpu_time = sum(gpu_time_per_batch)\n",
    "    print(f'Total GPU time: {total_gpu_time} seconds (~{total_gpu_time / 3600:.2f} hours)')\n",
    "\n",
    "    # Calculate average power consumption per GPU\n",
    "    if gpu_power_usage_per_batch:\n",
    "        avg_gpu_power_usage = [sum(power) / len(gpu_power_usage_per_batch) for power in zip(*gpu_power_usage_per_batch)]\n",
    "        print(f'Average GPU power consumption (per GPU): {avg_gpu_power_usage}')\n",
    "\n",
    "    # Calculate total energy consumption per GPU\n",
    "    total_energy_consumption = [\n",
    "        sum([power[i] * time / 3600 for power, time in zip(gpu_power_usage_per_batch, gpu_time_per_batch)])\n",
    "        for i in range(len(avg_gpu_power_usage))\n",
    "    ]\n",
    "    print(f'Total energy consumption (per GPU): {total_energy_consumption} Wh')\n",
    "    \n",
    "    for i,seq in enumerate(outputs):\n",
    "        with open(f'./output/RE/{i}.html','w',encoding='utf-8') as f_write:\n",
    "            f_write.write(seq.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13f122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memorization",
   "language": "python",
   "name": "memorization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
